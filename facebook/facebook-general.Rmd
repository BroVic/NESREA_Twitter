---
title: "Facebook"
author: "Web Monitoring Group"
date: 
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```
# Preliminaries
Load and attach packages, as well as custom functions
```{r dependencies, echo=TRUE, results='hide'}
suppressPackageStartupMessages(
  lapply(c("DBI", "RSQLite", "Rfacebook", "dplyr", "ggplot2"),
         library,
         character.only = TRUE)
)

source("fb-functions.R")
```

# Data collection
Fetch the Page's Newsfeed from the local database. A summary of the available data is displayed.
```{r retrieve-feed}
con <- dbConnect(SQLite(), "../data/nesreanigeria.db")
page_posts <- dbReadTable(con, "nesreanigeria_fbposts")
  
glimpse(page_posts)
```

Note that the function `getPage()` is used to collect these posts through the Facebook API/Rfacebook package. It can equally be used to harvest data from others' public posts via the `page` argument.


# Data cleaning  
We have a user-defined function we have called `prepare_data()` that lives in the file *fb-functions* that was sourced earlier and it will help us do some data cleaning very quickly.  

The function:

- takes a data frame as an argument
- checks that the column names match those of typical Facebook post data
- removes non-human readable characters from the 'message' variable
- changes 'type' from a character to a categorical variable
- converts 'time' into a Date-Time object (POSIX standard)
- returns the modified data frame  

This is what the function definition looks like:  
```{r, results='hold'}
prepare_data
```

Also columns 1 and 2 are redundant for the purpose of these particular analyses and will be removed. This is how it's done and the data now look like this  
```{r clean-data, echo=TRUE}
page_posts <- page_posts %>%
  prepare_data() %>%
  select(message:shares_count)

glimpse(page_posts)
```



```{r cleanup}
dbDisconnect(con)
suppressWarnings(rm(con))
```

